# SmartFirm.io - Robots.txt Configuration
# Allows all search engines to crawl the entire site

User-agent: *
Allow: /

# Block utility pages from indexing (these have noindex meta tags too)
Disallow: /404
Disallow: /500
Disallow: /thank-you
Disallow: /case-studies
Disallow: /success-stories

# Sitemap location
Sitemap: https://smartfirm.io/sitemap.xml

# ============================================
# PREVIEW & STAGING URL PROTECTION
# ============================================
#
# Note: Preview and branch deploy URLs (*.netlify.app) are protected
# via X-Robots-Tag HTTP headers in netlify.toml, not in robots.txt.
# This ensures search engines cannot index preview/staging environments.
#
# Headers configured:
# - deploy-preview: X-Robots-Tag = "noindex, nofollow, noarchive"
# - branch-deploy: X-Robots-Tag = "noindex, nofollow, noarchive"
# - production: X-Robots-Tag = "all"

# ============================================
# HOW TO MODIFY THIS FILE
# ============================================
# 
# To block specific pages or directories:
# Add lines like: Disallow: /page-path
# 
# Examples:
# Disallow: /admin          (blocks /admin and all subdirectories)
# Disallow: /tools/         (blocks all tools pages)
# Disallow: /draft-page     (blocks a specific page)
#
# To block specific search engines:
# User-agent: Googlebot
# Disallow: /private-section
#
# To allow everything for a specific bot:
# User-agent: Googlebot
# Allow: /
#
# Common user-agents:
# - Googlebot (Google)
# - Bingbot (Bing)
# - Slurp (Yahoo)
# - DuckDuckBot (DuckDuckGo)
# - Baiduspider (Baidu)
